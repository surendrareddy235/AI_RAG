ğŸ”¹ Imports
from langchain_community.document_loaders import PyPDFLoader
â†’ Loads and reads content from a PDF file.

from langchain.text_splitter import CharacterTextSplitter
â†’ Splits long text into smaller overlapping chunks.

from langchain_community.embeddings import HuggingFaceEmbeddings
â†’ Generates vector embeddings using a Hugging Face model.

from langchain_community.vectorstores import FAISS
â†’ Stores and retrieves vectors using the FAISS similarity search library.

import numpy as np
â†’ Imports NumPy (not used in this script directly, but commonly needed with embeddings).

from langchain.chains import RetrievalQA
-> used to chaining the answers and question embedding

from langchain_google_genai import ChatGoogleGenerativeAI
->this will you with interacting with google models
import os
-> in this project os will help you to get the env 

from dotenv import load_dotenv
->dotenv is where your api is stored than with the help of os you can get the api key

ğŸ”¹ PDF Loading
loader = PyPDFLoader('surya.pdf')
â†’ Initializes the PDF loader with your file.

document = loader.load()
â†’ Loads and parses the PDF into a list of document objects.

ğŸ”¹ Text Splitting
splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=10)
â†’ Sets the chunk size to 50 characters with a 10-character overlap between chunks.

text = splitter.split_documents(document)
â†’ Splits the entire PDF content into manageable text chunks.

ğŸ”¹ Embedding Generation
embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
â†’ Loads a lightweight and efficient sentence embedding model.
-> it is a sentence level embedding model it will create a vector size of 384 for each chunk i told you that chunk mean a sentence if a sentence have 1 word nun the less this model creates 384 vector size the same size of a full sentence with 50 words 


ğŸ”¹ Vector Store Creation
vector_store = FAISS.from_documents(text, embedding_model)
â†’ Converts the chunks into embeddings and stores them in a FAISS vector index.

ğŸ”¹ Accessing the FAISS Index
vector_embeddings = vector_store.index
â†’ Retrieves the raw FAISS index which contains all the vector embeddings.
-> A single vector of size 384 for each chunk.

 <!-- LLM Setup â€“ Gemini API -->

    llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash', google_api_key=os.getenv('GEMINI_API_KEY'))
    â†’ Initializes the Gemini LLM using your API key stored in .env.

ğŸ”¹ RetrievalQA Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_store.as_retriever(search_kwargs={"k": 2}),
    return_source_documents=True
)

â†’ Combines your retriever (FAISS) and Gemini LLM into a single question-answering chain.

âœ”ï¸ It searches the top 2 relevant chunks based on your query.
âœ”ï¸ It embeds your question internally, fetches relevant data, and sends it to Gemini for answering.

ğŸ”¹ Continuous Q&A Loop

while True:
    query = input("ask a question (or 'exit'):")
    if query.lower() == "exit":
        break
    response = qa_chain.invoke({'query': query})
    print('gemini:', response["result"])

â†’ This loop allows you to ask multiple questions continuously.
â†’ Type 'exit' to quit.
â†’ Gemini will answer based only on the content inside your PDF.